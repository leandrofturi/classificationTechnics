{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7V5Y4vFtenJ"
   },
   "source": [
    "# Segundo Trabalho de InteligÃªncia Artificial e Sistemas Inteligentes\n",
    "\n",
    "Leandro Furlam Turi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxVGuqDxtieS"
   },
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3014,
     "status": "ok",
     "timestamp": 1618346480824,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "fd1iiKz1AoHT"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W12_sRNWtmZ8"
   },
   "source": [
    "## Implementation of some methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUdPMOaWtqNB"
   },
   "source": [
    "### Probabilistic OneR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3011,
     "status": "ok",
     "timestamp": 1618346480825,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "7Ez_FSphFTqd"
   },
   "outputs": [],
   "source": [
    "'''OneR Algorithm\n",
    "For each predictor,\n",
    "     For each value of that predictor, make a rule as follows;\n",
    "           Count how often each value of target (class) appears\n",
    "           Find the most frequent class\n",
    "           Make the rule assign that class to this value of the predictor\n",
    "     Calculate the total error of the rules of each predictor\n",
    "Choose the predictor with the smallest total error.\n",
    "'''\n",
    "\n",
    "class OneRProbabilistic(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, disc=True):\n",
    "        self._disc = disc\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        # Discretization\n",
    "        self.__est = KBinsDiscretizer(n_bins=2*self.n_classes_, encode='ordinal', strategy='kmeans') if self._disc else None\n",
    "        X_disc = self.__est.fit_transform(X) if self._disc else X\n",
    "\n",
    "        # Contingency tables\n",
    "        contingency_tb = [pd.crosstab(x, y, rownames=['data'], colnames=['target']) for x in X_disc.T]\n",
    "        # Feature with greater power differentiation\n",
    "        self.__great_diff = np.argmax([tb.max(axis=1).sum() for tb in contingency_tb])\n",
    "        # Class distribution of each characteristic value\n",
    "        self.__dist_class = contingency_tb[self.__great_diff].div(contingency_tb[self.__great_diff].sum(axis=1), axis=0)\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Discretization\n",
    "        X_disc = self.__est.transform(X) if self._disc else X\n",
    "\n",
    "        # Input validation\n",
    "        X_disc = check_array(X_disc)\n",
    "\n",
    "        # roulette taking into account the distribution of classes\n",
    "        predicted = [self.__dist_class.columns[(self.__dist_class.loc[x].cumsum() >= np.random.rand()).idxmax()] for x in X_disc[:,self.__great_diff]]\n",
    "        return np.array(predicted)\n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return {'disc': self._disc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETxFLNOJtvug"
   },
   "source": [
    "### KCentroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1618349384291,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "VZ4GxtYOUpN5"
   },
   "outputs": [],
   "source": [
    "# KCentroids\n",
    "\n",
    "'''KCentroids\n",
    "The KCentroides classifier uses a grouping algorithm to define K groups of examples from each class in the training base.\n",
    "Assuming that a database has ncl classes, the KCentroides algorithm initially forms K * ncl groups, with K groups in each of the ncl classes.\n",
    "Then, the centroid of each group is calculated and this centroid is associated with the class of the group from which it was generated.\n",
    "The hyperparameter method has the value of K.\n",
    "To perform a classification, KCentroides checks which centroid is closest to the element to be classified and returns to its class.\n",
    "'''\n",
    "\n",
    "class KCentroids(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator='kmeans', argv=None, n_clusters=8):\n",
    "        self._estimator = estimator\n",
    "        self._n_clusters = n_clusters\n",
    "        self._argv = argv\n",
    "        if estimator == 'kmeans':\n",
    "            self._est = KMeans(n_clusters)\n",
    "        elif estimator == 'ga':\n",
    "            self._est = GA(n_clusters, argv)\n",
    "        else:\n",
    "            raise Exception('Estimator not definied!')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        # Geting centroids with estimator\n",
    "        self.__groups = [self._est.fit(X[y == c]).cluster_centers_ for c in self.classes_]\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        predicted = [self.classes_[np.argmin([min([np.linalg.norm(x - c) for c in g]) for g in self.__groups])] for x in X]\n",
    "        return np.array(predicted)\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return {'estimator': self._estimator,\n",
    "                'n_clusters': self._n_clusters,\n",
    "                'argv': self._argv}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNt0iHuRt31b"
   },
   "source": [
    "#### Genetic Algorithm (from T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1618349390949,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "iNZGSlx4FTx7"
   },
   "outputs": [],
   "source": [
    "class GA():\n",
    "    '''Genetic Algorithm\n",
    "    The initial population is constructed by hill climbing algorithm. \n",
    "    The selection mechanism is given by tournament, where a number of candidates for parents are put forward and only the best two are selected to generate offspring.\n",
    "    Since the crossing mechanism selects only two individuals, this step is repeated pop_size/2 times, so that all individuals have an equal chance of performing the recombination.\n",
    "    The combination mechanism used was also highly inspired by nature, to which the father donates a sperm that will be fertilized in an egg donated by the mother.\n",
    "    Technically, the father donates a randomly chosen cluster and it is inserted into the mother. \n",
    "    Then, the items present in the cluster donated by the father are removed from the mother cluster generating a new individual. \n",
    "    If it occurs to result in the child more clusters than necessary, those with the least amount of items are grouped; otherwise, the cluster with the largest amount are separated in half.\n",
    "    This technique was adopted so that good groupings can be maintained.\n",
    "    The mutation mechanism applied is strong: if the individual is selected, he is discarded and a new individual is inserted into the population.\n",
    "    Note, therefore, that the mutation occurs more in the population than in the individual.\n",
    "    As for the offspring, only the pop_size best individuals are allowed to enter a new generation.\n",
    "    As a stopping criterion, maximum number of iterations, maximum processing time and whether the population has converged, that is, if all individuals are identical, are used.\n",
    "    '''\n",
    "    def __init__(self, n_clusters, argv):\n",
    "        self.k_ = n_clusters\n",
    "        self.pop_size_ = argv['pop_size']\n",
    "        self.iter_max_ = argv['iter_max']\n",
    "        self.cross_ratio_ = argv['cross_ratio']\n",
    "        self.mut_ratio_ = argv['mut_ratio']\n",
    "        self.max_time_ = argv['max_time']\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Check that X have correct shape\n",
    "        X = check_array(X)\n",
    "\n",
    "        self.X_ = X\n",
    "\n",
    "        # Geting centroids with estimator\n",
    "        solution = self.genetic(X, self.k_, self.pop_size_, self.iter_max_, \n",
    "                                       self.cross_ratio_, self.mut_ratio_, self.max_time_)\n",
    "        self.cluster_centers_ = solution['cluster_centers_'],\n",
    "        self.sse_ = solution['sse_'],\n",
    "        self.clusters_ = solution['clusters_']\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "\n",
    "        predicted = [self.classes_[np.argmin([np.linalg.norm(x - g) for g in self.__solution['cluster_centers_']])] for x in X]\n",
    "        return np.array(predicted)\n",
    "\n",
    "\n",
    "    def __available_closest_items(self, cluster, available_items, n):\n",
    "        # n items more closet to centroid of cluster\n",
    "        n = min(n, len(available_items))\n",
    "        evaluations = [self.__evaluate_cluster(cluster + [i]) for i in available_items]\n",
    "        closest = np.argpartition([e['sum_dist'] for e in evaluations], range(n))[:n]\n",
    "        return closest\n",
    "\n",
    "    def hill_climbing(self, items, k):\n",
    "        num_best = k\n",
    "        available_items = list(range(len(items)))\n",
    "        np.random.shuffle(available_items)\n",
    "        # choose the firsts items of clusters randomly\n",
    "        clusters = [[items[available_items.pop()]] for _ in range(k)]\n",
    "\n",
    "        current_cluster = 0\n",
    "        while available_items:\n",
    "            closest = self.__available_closest_items(clusters[current_cluster], [items[p] for p in available_items], num_best)\n",
    "            choiced = available_items[closest[np.random.randint(len(closest))]]\n",
    "            clusters[current_cluster].append(items[choiced])\n",
    "            available_items.remove(choiced)\n",
    "            current_cluster = (current_cluster + 1) % k\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def __evaluate_cluster(self, cluster):\n",
    "        # euclidean norm\n",
    "        items = [s['coord'] for s in cluster]\n",
    "        mu = np.average(items, axis=0) # centroid\n",
    "        \n",
    "        norm2 = [np.linalg.norm(i-mu) for i in items]\n",
    "        return {'sum_dist': sum(norm2), 'dist': norm2, 'mu': mu.tolist(), 'items': cluster}\n",
    "\n",
    "    def __evaluate_clusters(self, clusters):\n",
    "        states = [self.__evaluate_cluster(cluster) for cluster in clusters]\n",
    "        return states\n",
    "\n",
    "    def __objective_function(self, states):\n",
    "        # SSE metric\n",
    "        sse = sum([state['sum_dist'] for state in states])\n",
    "        return sse\n",
    "\n",
    "    def __random_state(self, states):\n",
    "        n = len(states)-1\n",
    "        if n <= 1:\n",
    "            return states[0]\n",
    "\n",
    "        index = np.random.randint(0, n)\n",
    "        return states[index]\n",
    "\n",
    "    def __initial_population(self, n, items, k):\n",
    "        pop = [self.__evaluate_clusters(self.hill_climbing(items, k)) for _ in range(n)]\n",
    "        return pop\n",
    "\n",
    "    def __selection(self, population, n, n_tournament):\n",
    "        # by tournament\n",
    "        N = len(population)\n",
    "        if n >= N:\n",
    "            return population\n",
    "        if n_tournament > N:\n",
    "            n_tournament = N\n",
    "\n",
    "        selecteds = np.random.choice(range(0, len(population)), size=n_tournament, replace=False)\n",
    "        selecteds.sort()\n",
    "        values = [self.__objective_function(population[i]) for i in selecteds]\n",
    "        pos_ordered = np.argpartition(values, range(n))[-n:]\n",
    "        return [population[p] for p in pos_ordered]\n",
    "\n",
    "    def __crossover(self, dad, mom):\n",
    "        # son product of an egg fertilized by a sperm\n",
    "        sperm = self.__random_state(dad)['items']\n",
    "        # take all groups that not contain the sperm, and remove it too\n",
    "        egg = [e for e in [[n for n in m['items'] if n not in sperm] for m in mom] if e]\n",
    "        son = egg + [sperm]\n",
    "\n",
    "        k = len(dad)\n",
    "        while len(son) > k: # concatenate smaller groups\n",
    "            smallers = np.argpartition([len(s) for s in son], range(2))[:2]\n",
    "            son[smallers[0]] += son[smallers[1]]\n",
    "            del son[smallers[1]]\n",
    "\n",
    "        while len(son) < k: # separate larger groups\n",
    "            larger = np.argmax([len(s) for s in son])\n",
    "            half = len(son[larger])//2\n",
    "            son += [son[larger][:half]] + [son[larger][half:]]\n",
    "            del son[larger]\n",
    "\n",
    "        return self.__evaluate_clusters(son)\n",
    "\n",
    "    def __mutation(self, items, k):\n",
    "        # new individual\n",
    "        return self.__evaluate_clusters(self.hill_climbing(items, k))\n",
    "\n",
    "    def __convergent(self, population):\n",
    "        clusters_0 = [sorted([i['id'] for i in p['items']]) for p in population[0]]\n",
    "        clusters = [[sorted([i['id'] for i in clusters['items']]) for clusters in states] for states in population]\n",
    "        return all(c == clusters_0 for c in clusters)\n",
    "\n",
    "    def __evaluate_population(self, population):\n",
    "        return sum([self.__objective_function(p) for p in population], [])\n",
    "\n",
    "    def __offspring(self, population, n):\n",
    "        best_index = np.argpartition([sum([q['sum_dist'] for q in p]) for p in population], range(n))[:n]\n",
    "        return [population[i] for i in best_index]\n",
    "\n",
    "    def genetic(self, X, k, pop_size, iter_max, cross_ratio, mut_ratio, max_time):\n",
    "        items = [{'id': x, 'coord': y} for x, y in zip(range(len(X)), X)]\n",
    "        n_tournament = 3\n",
    "        half_pop = pop_size//2\n",
    "        pop = self.__initial_population(pop_size, items, k)\n",
    "        iter = 0\n",
    "        end = 0\n",
    "\n",
    "        start = time.process_time()\n",
    "        while True:\n",
    "            new_pop = pop.copy()\n",
    "            for _ in range(half_pop): # everyone can cross\n",
    "                if np.random.uniform(0, 1, 1) <= cross_ratio:\n",
    "                    parents = self.__selection(pop, 2, n_tournament)\n",
    "                    new_pop.append(self.__crossover(parents[0], parents[1]))\n",
    "                if np.random.uniform(0, 1, 1) <= mut_ratio:\n",
    "                    new_pop.append(self.__mutation(items, k))\n",
    "            pop = self.__offspring(new_pop, pop_size)\n",
    "            val_pop = self.__evaluate_population(pop)\n",
    "\n",
    "            iter += 1\n",
    "            end = time.process_time()\n",
    "            if iter >= iter_max:\n",
    "                br = 'iteration'\n",
    "                break\n",
    "            if end-start > max_time:\n",
    "                br = 'time'\n",
    "                break\n",
    "            if self.__convergent(pop):\n",
    "                br = 'convergence'\n",
    "                break\n",
    "\n",
    "        best_individual = self.__offspring(pop, 1)[0]\n",
    "        return {'cluster_centers_': [b['mu'] for b in best_individual],\n",
    "                'sse_': [b['sum_dist'] for b in best_individual],\n",
    "                'clusters_': [[c['coord'] for c in b['items']] for b in best_individual]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CrtsLbHuLRq"
   },
   "source": [
    "## Search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1618349397319,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "tCFQS-egPOvn"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'ZeroR': DummyClassifier(strategy='most_frequent'),\n",
    "    'Random': DummyClassifier(strategy='uniform'),\n",
    "    'Stratified Random': DummyClassifier(strategy='stratified'),\n",
    "    'OneR': OneRProbabilistic(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'KmeansCentroids': KCentroids('kmeans'),\n",
    "    'KGACentroids': KCentroids('ga', {'pop_size':50, 'iter_max':100,'cross_ratio':0.75, 'mut_ratio':0.2, 'max_time':1}),\n",
    "    'Knn': KNeighborsClassifier(weights='uniform'),\n",
    "    'DistKnn': KNeighborsClassifier(weights='distance'),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'KmeansCentroids': {'n_clusters': [1, 3, 5, 7]},\n",
    "    'KGACentroids': {'n_clusters': [1, 3, 5, 7]},\n",
    "    'Knn': {'n_neighbors': [1, 3, 5, 7]},\n",
    "    'DistKnn': {'n_neighbors': [1, 3, 5, 7]},\n",
    "    'Decision Tree': {'max_depth': [None, 3, 5, 10]},\n",
    "    'Random Forest': {'n_estimators': [10, 20, 50, 100]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEUYEOpcuQ7a"
   },
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1618347066361,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "oW73UyI8SgiF"
   },
   "outputs": [],
   "source": [
    "def experimental(base, filename):\n",
    "    X, y = iris.data, iris.target\n",
    "    print()\n",
    "    print(filename)\n",
    "    ans = {}\n",
    "    for c in ['ZeroR', 'Random', 'Stratified Random', 'OneR', 'Gaussian Naive Bayes']:\n",
    "        start = time.process_time()\n",
    "        pipeline = Pipeline([('transformer', StandardScaler()), ('estimator', models[c])])\n",
    "        ans[c] = cross_validate(pipeline, X, y, \n",
    "                                cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234), scoring='accuracy')\n",
    "        print('Elapsed time of {} is {:.6f} seconds.'.format(c, time.process_time() - start))\n",
    "\n",
    "    for c in ['KmeansCentroids', 'KGACentroids', 'Knn', 'DistKnn', 'Decision Tree', 'Random Forest']:\n",
    "        start = time.process_time()\n",
    "        clf = Pipeline([('transformer', StandardScaler()), \n",
    "                        ('estimator', GridSearchCV(models[c], param_grid=parameters[c], scoring='accuracy', \n",
    "                                                   cv=RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=36851234)))]).fit(X, y) # Grid search\n",
    "        pipeline = Pipeline([('transformer', StandardScaler()), ('estimator', clf)])\n",
    "        ans[c] = cross_validate(pipeline, X, y, \n",
    "                                cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=36851234), scoring='accuracy')\n",
    "        print('Elapsed time of {} is {:.6f} seconds.'.format(c, time.process_time() - start))\n",
    "\n",
    "    pickle.dump(ans, open(filename, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxU_kjYeuXLD"
   },
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "executionInfo": {
     "elapsed": 110121,
     "status": "error",
     "timestamp": 1618348523192,
     "user": {
      "displayName": "Leandro Furlam Turi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giexbi5AM6GAXth3e0t2Z0QXQKOp9AAbTIPTXd8=s64",
      "userId": "07988333438481462003"
     },
     "user_tz": 180
    },
    "id": "rRkxWadUIOaE",
    "outputId": "afee9494-6719-48f3-abb9-470d392daa8f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "wine = datasets.load_wine()\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "for k, base in {'iris':iris, 'digits':digits, 'wine':wine, 'cancer':cancer}.items():\n",
    "    experimental(base, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JOVIvsZuaGq"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLTUtdk8ZWRg"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"iris\"\n",
    "ans = pickle.load(open(filename, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.DataFrame()\n",
    "for a in ans.keys():\n",
    "    DF[a] = ans[a]['test_score']\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'mean': DF.mean(), 'std': DF.std(), \n",
    "              'confit_inf': DF.apply(lambda x: st.t.interval(0.95, len(x)-1, loc=np.mean(x), scale=st.sem(x))[0]),\n",
    "              'confit_sup': DF.apply(lambda x: st.t.interval(0.95, len(x)-1, loc=np.mean(x), scale=st.sem(x))[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.boxplot(data=DF)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "M = [[-1]*len(ans) for _ in range(len(ans))]\n",
    "\n",
    "for i in range(len(ans)):\n",
    "    for j in range(i+1, len(ans)):\n",
    "        # Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
    "        M[i][j] = stats.ttest_rel(DF.iloc[:,i], DF.iloc[:,j]).pvalue\n",
    "        if any((DF.iloc[:,i] - DF.iloc[:,j]) != 0):\n",
    "            # Calculate the Wilcoxon signed-rank test.\n",
    "            M[j][i] = stats.wilcoxon(DF.iloc[:,i], DF.iloc[:,j]).pvalue\n",
    "pd.DataFrame(M, index=DF.columns, columns=DF.columns)\n",
    "# Reject when p-value is greater then 0.05"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN+/vUB1nk8vamMXsbJff1F",
   "collapsed_sections": [],
   "name": "T2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
